\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,booktabs,graphicx,xcolor}
\usepackage{hyperref}
\definecolor{svdblue}{HTML}{58a6ff}
\definecolor{humgreen}{HTML}{3fb950}

\title{The Event Space Isomorphism:\\Arch-Native and Human-Native Partitions\\of the Byte Alphabet}
\author{Claude and MJC}
\date{February 11, 2026}

\begin{document}
\maketitle

\begin{abstract}
We show that the byte partition discovered by SVD of skip-bigram tables (the
\emph{arch-native} event space) and the partition defined by human knowledge of
byte semantics (the \emph{human-native} event space) are related by a
permutation that approximately preserves the inner product structure of the
reduced SVD subspace.  On the output side (right singular vectors), the
optimal permutation achieves up to 85.7\% frequency-weighted accuracy and
0.661 NMI.  On the input side, the arch-native partition is a \emph{refinement}
of the human-native partition: it discovers sub-category structure (vowel vs.\
consonant, mid-word vs.\ boundary) within the dominant ``lowercase'' class that
the human labeling does not distinguish.  This asymmetry is the E$\to$N map
made concrete: the arch-native events \emph{are} the numbers, and the human
labels are a coarsening.
\end{abstract}

\section{Two Partitions of $\{0,\ldots,255\}$}

\subsection{Arch-native: SVD sign bits}

Given a skip-bigram count table $C_{xy}^{(g)}$ at offset $g$, we form the
centered conditional matrix $A_{xo} = P(o|x) - P(o)$ and compute its SVD.
The top 3 left singular vectors $u_0, u_1, u_2 \in \mathbb{R}^{256}$ define
a partition of input bytes into $2^3 = 8$ groups by the sign pattern
$(\mathrm{sgn}\,u_0(b),\, \mathrm{sgn}\,u_1(b),\, \mathrm{sgn}\,u_2(b))$.
Similarly, the right singular vectors $v_0, v_1, v_2$ partition \emph{output}
bytes into 8 groups.  These are the \textbf{arch-native} partitions: they are
what the data's statistical structure discovers without human labels.

\subsection{Human-native: semantic byte classes}

We define 8 groups from common knowledge of the ASCII/byte encoding:
\begin{center}
\begin{tabular}{clrl}
\toprule
Group & Name & \#Bytes & Data fraction \\
\midrule
H0 & lowercase (a--z) & 26 & 67.3\% \\
H1 & uppercase (A--Z) & 26 & 3.5\% \\
H2 & digits (0--9) & 10 & 2.3\% \\
H3 & whitespace (space, \textbackslash n, tab) & 3 & 15.5\% \\
H4 & XML markup ($<$, $>$, /, \&) & 4 & 2.7\% \\
H5 & punctuation (., ; : ! ? ' " - etc.) & 13 & 6.9\% \\
H6 & other printable (= + * | \# etc.) & 15 & 1.3\% \\
H7 & non-printable (control, high bytes) & 159 & 0.5\% \\
\bottomrule
\end{tabular}
\end{center}
These labels require zero computation---they come from shared human background
knowledge of how text works.

\section{The Permutation}

For each offset $g \in \{0,\ldots,15\}$, we seek a permutation
$\pi_g: \{0,\ldots,7\} \to \{0,\ldots,7\}$ that maximizes the
frequency-weighted accuracy
\[
  \mathrm{Acc}(\pi) = \sum_{i=0}^{7} \sum_{b:\, \mathrm{SVD}(b)=i,\, \mathrm{Human}(b)=\pi(i)} \frac{f_b}{N}
\]
where $f_b$ is the count of byte $b$ in the data.  Since $8! = 40{,}320$,
we find the global optimum by brute force.

We also compute the \textbf{Normalized Mutual Information} (NMI) between the
two partitions (frequency-weighted), which is permutation-invariant and
measures how much knowing one partition tells you about the other, and the
\textbf{centroid cosine similarity}: the mean cosine between the
frequency-weighted centroid of each SVD group and its matched human group,
projected into the 3-dimensional SVD subspace.

\section{Results}

\subsection{Output side (V): strong isomorphism}

\begin{center}
\begin{tabular}{rrrrl}
\toprule
Offset & Acc\% & NMI & Cos & Notable mapping \\
\midrule
 0 & 60.3 & 0.604 & 0.225 & lower$\leftrightarrow$SVD3, ws$\leftrightarrow$SVD5 \\
 1 & 73.9 & 0.639 & 0.553 & lower$\leftrightarrow$SVD7, ws$\leftrightarrow$SVD1 \\
 3 & 75.2 & 0.547 & 0.339 & lower$\leftrightarrow$SVD1, ws$\leftrightarrow$SVD7 \\
 4 & 68.9 & 0.532 & 0.395 & lower$\leftrightarrow$SVD5, ws$\leftrightarrow$SVD7 \\
 9 & \textbf{82.3} & 0.607 & 0.716 & lower$\leftrightarrow$SVD7, ws$\leftrightarrow$SVD3 \\
10 & 70.9 & 0.590 & 0.407 & lower$\leftrightarrow$SVD1, punct$\leftrightarrow$SVD0 \\
11 & \textbf{85.7} & \textbf{0.661} & 0.542 & lower$\leftrightarrow$SVD7, ws$\leftrightarrow$SVD1 \\
13 & 68.1 & 0.600 & 0.801 & lower$\leftrightarrow$SVD7, ws$\leftrightarrow$SVD1 \\
\midrule
\textbf{Mean (all 16)} & \textbf{62.2} & \textbf{0.524} & \textbf{0.504} & \\
\bottomrule
\end{tabular}
\end{center}

At offset 11, 85.7\% of all byte occurrences are correctly classified under
the optimal permutation.  The NMI of 0.661 means the two partitions share
about two-thirds of their information content.

\subsection{Input side (U): refinement}

\begin{center}
\begin{tabular}{rrrrp{5.5cm}}
\toprule
Offset & Acc\% & NMI & Cos & Key structure \\
\midrule
 0 & 40.7 & 0.359 & 0.371 & SV1 splits lower into word-interior vs.\ boundary \\
 1 & 47.9 & 0.375 & 0.395 & 5 SVD groups contain lowercase subsets \\
 4 & \textbf{55.1} & 0.387 & \textbf{0.884} & Best centroid alignment; ws perfectly isolated \\
 8 & 46.6 & 0.323 & 0.350 & ws perfectly isolated (SVD7 = 14.2\%) \\
11 & 46.4 & 0.428 & 0.838 & digits isolated; ws isolated \\
12 & 50.4 & 0.387 & 0.659 & lowercase split: 30\% + 17\% in two groups \\
\midrule
\textbf{Mean (all 16)} & \textbf{44.3} & \textbf{0.344} & \textbf{0.514} & \\
\bottomrule
\end{tabular}
\end{center}

Input-side accuracy is systematically lower (44\% vs.\ 62\%).  The reason is
clear from the confusion matrices: \textbf{lowercase letters (67\% of data)
are split across 3--5 SVD groups}, each capturing a different predictive
feature.  For instance, at offset~0:
\begin{itemize}
  \item SVD group 7: 21.0\% lowercase (the ``text continuation'' group)
  \item SVD group 2: 13.8\% lowercase (the ``XML-adjacent text'' group)
  \item SVD group 5: 13.8\% lowercase (the ``word-internal'' group)
\end{itemize}

Meanwhile, the smaller human classes map cleanly:
\begin{itemize}
  \item \textbf{Whitespace} is perfectly isolated into a single SVD group at
    11 of 16 offsets (the group contains $\geq$14.2\% ws and $<$1\% non-ws).
  \item \textbf{Digits} are isolated at offsets 0, 1, 9, 11, 14, 15.
  \item \textbf{Non-printable} bytes always map to a single group (0.5\% of data).
\end{itemize}

\section{The Asymmetry Is the E$\to$N Map}

The key observation: the input-side arch-native partition is a \emph{refinement}
of the human-native partition.  Every SVD group is approximately a subset of
one human class.  The human partition is obtained by \emph{merging} SVD groups:

\[
  \underbrace{\text{SVD}_2 \cup \text{SVD}_5 \cup \text{SVD}_7}_{\text{``lowercase''}}
  \quad
  \underbrace{\text{SVD}_6}_{\text{``uppercase''}}
  \quad
  \underbrace{\text{SVD}_4 \cap \text{digits}}_{\text{``digits''}}
  \quad
  \underbrace{\text{SVD}_4 \cap \text{ws}}_{\text{``whitespace''}}
\]

This is exactly the \textbf{E$\to$N} map from our framework.  The arch-native
events (E) are the fine-grained statistical reality.  The human-native labels
(N) are a numbering system---a coarsening that assigns names to clusters of
events.  The permutation $\pi$ is not a bijection between equal partitions;
it is the best \emph{alignment} of a fine partition with a coarse one.

On the output side, the asymmetry reverses: the human-native partition is
closer to the arch-native one because output bytes are classified by ``what
predicts me'' (the distributional context), which closely tracks human
categories.  Space is predicted after word-ending letters; digits after
other digits or colons; uppercase after periods and newlines.  The
\emph{distributional definition} of a category matches the \emph{semantic
definition}.

\section{Inner Product Preservation}

In the 3-dimensional SVD subspace, each byte $b$ has coordinates
$(u_0(b), u_1(b), u_2(b))$.  The frequency-weighted centroid of each group
defines a point in this space.  If the isomorphism preserves inner product
structure, then the centroid of SVD group $i$ should be close to the centroid
of human group $\pi(i)$.

We measure this by the cosine similarity between paired centroids.  On the
input side, the mean cosine across all offsets is \textbf{0.514} (range:
0.195--0.884).  On the output side, the mean is \textbf{0.504} (range:
0.113--0.863).

The best cases (cosine $>$ 0.8) occur at offsets 4, 11, 12 (input) and
offsets 7, 9, 13 (output).  These are the offsets where the SVD's statistical
partition most closely aligns with human semantic intuition.

The moderate mean cosine ($\approx 0.5$) reflects the refinement structure:
SVD centroids for lowercase subgroups point in \emph{different} directions
within the lowercase region, while the human ``lowercase'' centroid averages
them.  The alignment is partial because the SVD resolves structure that the
human labeling collapses.

\section{Pair Agreement}

For all pairs of active bytes, we ask: do the two partitions agree on whether
they belong to the same group?

\begin{center}
\begin{tabular}{lrr}
\toprule
 & Input (U) & Output (V) \\
\midrule
Same-group agreement & 44.3\% & --- \\
Different-group agreement & 76.4\% & --- \\
\bottomrule
\end{tabular}
\end{center}

The asymmetry is diagnostic: the partitions rarely \emph{disagree} about bytes
being in different groups (76\% agreement), but the arch-native partition splits
groups that the human partition merges (only 44\% same-group agreement).
This confirms the refinement interpretation.

\section{Conclusion}

The arch-native (SVD) and human-native (semantic) event spaces are not
identical partitions, but they are related by a permutation that preserves
the essential structure.  The V-side (output) isomorphism is strong:
85.7\% accuracy at best, 62\% mean.  The U-side (input) is a refinement:
the SVD discovers sub-category structure within the human ``lowercase'' class
that reflects genuine predictive distinctions (vowel/consonant, word
position, XML context).

This is the E$\to$N map made explicit.  The events are the arch-native
statistical reality; the numbers are the human labels we assign to make
them intelligible.  The permutation $\pi$ is not arbitrary---it consistently
maps whitespace to whitespace, digits to digits, XML to XML.  The fact
that it also discovers that ``lowercase'' is not one event but several is
not a failure of the isomorphism; it is the isomorphism \emph{doing its job},
revealing structure that human labeling merely approximates.

\vspace{1em}
\noindent\textbf{Data:} 262,144 bytes of enwik9.  16 offsets.  SVD: 300
power iterations, top 8 components.  Permutation: brute-force 8!.
Tool: \texttt{es\_iso.c}.

\end{document}
